import matplotlib.pyplot as plt
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
import tensorflow as tf
!pip install seaborn
import seaborn as sns

from sklearn.model_selection import  train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import cohen_kappa_score, accuracy_score
from keras.applications import DenseNet121
from keras import layers
from keras.models import Sequential
from keras.optimizers import Adam
from keras.callbacks import Callback, ModelCheckpoint
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt

#we are doing exploratory data analysis
#we need data to find outliers in our data 
import os 
directory= '/kaggle/input/diabetic-retinopathy-balanced/content/Diabetic_Balanced_Data/test/0'
images =[]
image_arrays =[]
for filename in os.listdir(directory):
    if filename.endswith(".jpeg") or filename.endswith('.jpg'):
        image_path =  os.path.join(directory, filename)
        img = image.load_img(image_path, target_size=(224, 224))#maintain height and width of the image 
        #converting the image into numpy array
      
        image_array = image.img_to_array(img)
          # Append the image and its array to the lists
        images.append(img)
        image_arrays.append(image_array)

# Convert the list of arrays to a numpy array
test_arrays = np.array(image_arrays)

# Print the shape of the numpy array containing all images
print("Shape of test_arrays:", test_arrays.shape)

#we need to find array for train as well as validation data set so we will run the same code 
#train set -----
directory= '/kaggle/input/diabetic-retinopathy-balanced/content/Diabetic_Balanced_Data/train/0'
images =[]
image_arrays =[]
for filename in os.listdir(directory):
    if filename.endswith(".jpeg") or filename.endswith('.jpg'):
        image_path =  os.path.join(directory, filename)
        img = image.load_img(image_path, target_size=(224, 224))#maintain height and width of the image 
        #converting the image into numpy array
      
        image_array = image.img_to_array(img)
          # Append the image and its array to the lists
        images.append(img)
        image_arrays.append(image_array)

# Convert the list of arrays to a numpy array
train_arrays = np.array(image_arrays)

# Print the shape of the numpy array containing all images
print("Shape of train_arrays:", train_arrays.shape)


#validation 

directory= '/kaggle/input/diabetic-retinopathy-balanced/content/Diabetic_Balanced_Data/val/0'
images =[]
image_arrays =[]
for filename in os.listdir(directory):
    if filename.endswith(".jpeg") or filename.endswith('.jpg'):
        image_path =  os.path.join(directory, filename)
        img = image.load_img(image_path, target_size=(224, 224))#maintain height and width of the image 
        #converting the image into numpy array
      
        image_array = image.img_to_array(img)
          # Append the image and its array to the lists
        images.append(img)
        image_arrays.append(image_array)

# Convert the list of arrays to a numpy array
val_arrays = np.array(image_arrays)

# Print the shape of the numpy array containing all images
print("Shape of val_arrays:", val_arrays.shape)


# Set the paths to your dataset folders
train_dir = "/kaggle/input/diabetic-retinopathy-balanced/content/Diabetic_Balanced_Data/train/0"
validation_dir = "/kaggle/input/diabetic-retinopathy-balanced/content/Diabetic_Balanced_Data/val/0"
test_dir = "/kaggle/input/diabetic-retinopathy-balanced/content/Diabetic_Balanced_Data/test/0"

# Parameters
img_height = 224
img_width = 224
batch_size = 16

# Load datasets
train_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    train_dir,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    labels=[0] * len(os.listdir(train_dir))
)

validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    validation_dir,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    labels=[0] * len(os.listdir(validation_dir))
   
)

test_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    test_dir,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    labels=[0] * len(os.listdir(test_dir))
   
)

# Data augmentation
data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip('horizontal'),
    tf.keras.layers.RandomRotation(0.2),
    tf.keras.layers.RandomZoom(0.2),
])

# Apply augmentation to training dataset
train_dataset = train_dataset.map(
    lambda x, y: (data_augmentation(x, training=True), y))

# Optimize datasets for performance
train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

# Print dataset details
print(f"Training set cardinality: {tf.data.experimental.cardinality(train_dataset).numpy()}")
print(f"Validation set cardinality: {tf.data.experimental.cardinality(validation_dataset).numpy()}")
print(f"Test set cardinality: {tf.data.experimental.cardinality(test_dataset).numpy()}")

#exploratory data analysis for top 3 images
# Define a function to visualize images
import matplotlib.cm as cm 
cmap = cm.get_cmap('RdBu_r', 256) 
def visualize_images(dataset, num_images=3, figsize=(10, 10)):

    for i, (image_batch, label_batch) in enumerate(dataset.take(1)):  # Take 1 batch

        for j in range(num_images):  # Iterate over the batch dimension

            plt.subplot(1, num_images, j+1)
       

            image = image_batch[j].numpy().astype(np.float32)

            plt.imshow(image, vmin=0.0, vmax=1.0,cmap =cmap)

           

            plt.axis('off')

    plt.show()


# Visualize top 3 images from each dataset

print("Training dataset:")

visualize_images(train_dataset)


print("Test dataset:")

visualize_images(test_dataset)


print("Validation dataset:")

visualize_images(validation_dataset)



import matplotlib.pyplot as plt

def plot_images(images, labels, num=5):
    plt.figure(figsize=(20, 20))
    for i in range(num):
        plt.subplot(1, num, i+1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(labels[i].numpy())
        plt.axis("off")
    plt.show()

for images, labels in train_dataset.take(1):
    plot_images(images, labels)


#calculating mean and standard deviation 
import numpy as np

def calculate_stats(dataset):
    pixel_values = np.concatenate([x.numpy().flatten() for x, y in dataset])
    mean = np.mean(pixel_values)
    std = np.std(pixel_values)
    return mean, std

train_mean, train_std = calculate_stats(train_dataset)
print(f"Train dataset - Mean: {train_mean}, Std: {train_std}")

val_mean, val_std = calculate_stats(validation_dataset)
print(f"Validation dataset - Mean: {val_mean}, Std: {val_std}")

test_mean, test_std = calculate_stats(test_dataset)
print(f"Test dataset - Mean: {test_mean}, Std: {test_std}")

#finding outliers 
import seaborn as sns

def plot_distribution(dataset, title):
    pixel_values = np.concatenate([x.numpy().flatten() for x, y in dataset])
    sns.histplot(pixel_values, bins=50, kde=True)
    plt.title(title)
    plt.show()

plot_distribution(train_dataset, "Train Dataset Pixel Value Distribution")
plot_distribution(validation_dataset, "Validation Dataset Pixel Value Distribution")
plot_distribution(test_dataset, "Test Dataset Pixel Value Distribution")


from tensorflow.keras import layers, models

model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
history = model.fit(
    train_dataset,
    validation_data=validation_dataset,
    epochs=10
)

#model evaluation 
val_loss, val_acc = model.evaluate(validation_dataset)
print(f"Validation Accuracy: {val_acc*100:.2f}%")

test_loss, test_acc = model.evaluate(test_dataset)
print(f"Test Accuracy: {test_acc*100:.2f}%")
#saving the model 
model.save('diabetic_retinopathy_model.h5')

#making prediction s
new_images = [...]  # Load or preprocess new images here
predictions = model.predict(new_images)

# Convert predictions to binary labels
predicted_labels = [1 if pred > 0.5 else 0 for pred in predictions]
print(predicted_labels)
